{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "&emsp;&emsp;logistic regression逻辑回归算法可以说是机器学习中十分常见和实用性极强的算法了。因为其参数的可解释性，逻辑回归在工业界中受到青睐。比如，在金融征信相关的模型中，我们需要明确解释某个变量在变化时，其最终信用的变化趋势，我们叫做业务可解释性。下面，我们就直接进入正题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LR算法是对二分类问题一种回归分析预测方法。我们从以下步骤进行回顾：\n",
    "- 假设空间的函数\n",
    "- 目标（损失）函数\n",
    "- 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid函数\n",
    "&emsp;&emsp;假设模型参数为 $\\{\\alpha_{0},alpha_{1},...,alpha_{n}\\}$ ,其中 $\\alpha_{0}$ 为截距。可以用以下公式来表示y=1事件的概率：$$P(y=1)=\\frac{exp(\\alpha_{0}+\\alpha_{1}x_{1}+...+\\alpha_{n}x_{n})}{1+exp(\\alpha_{0}+\\alpha_{1}x_{1}+...+\\alpha_{n}x_{n})}$$\n",
    "\n",
    "转换上式形式，并令 $z=\\alpha_{0}+\\alpha_{1}x_{1}+...+\\alpha_{n}x_{n}$，则可以转换成sigmoid函数格式：$$sigmoid=\\frac{1}{1+exp(-z)}$$\n",
    "它表示y=1事件的概率p。再对其进行简单的转换就可以得到：$$\\ln{\\frac{p}{1-p}}=z$$\n",
    "公式 $\\frac{p}{1-p}$称为比率，表示y=1的概率与y=0概率的比值。LR算法可以看成是用**比率的对数作为因变量的线性回归模型**。  \n",
    "&emsp;&emsp;sigmoid函数可以得到一个概况预测值，另外其数学性质良好，是任意阶可导的凸函数，对于许多优化方法都是可使用的。在传统的神经网络中，sigmoid也曾作为神经元激活函数使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 似然估计与损失函数\n",
    "&emsp;&emsp;根据sigmoid函数，假设y=1时预测概率为: $$P(y=1|x)=h_{\\alpha}(x)=\\frac{1}{1+exp(-\\alpha^{T}x)}$$ 假设y服从伯努利分布，则 y=0时概率为: $$P(y=0|x)=1-h_{\\alpha}(x)$$ \n",
    "我们可以合并为： $$P(y|x)=h_{\\alpha}(x)^{y}(1-h_{\\alpha}(x))^{1-y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据上式我们就可以构造出该分布的似然函数，然后运用最大似然估计思想来求解其参数，我们根据上式写出其极大似然函数：\n",
    "$$\\begin{eqnarray}\n",
    "L(\\alpha) &=& \\prod_{i=1}^{m}p(y^{i}|x^{i};\\theta) \\\\\n",
    "&=& \\prod_{i=1}^{m}h_{\\alpha}(x^{i})^{y^{i}}(1-h_{\\alpha}(x^{i}))^{1-y^{i}}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "两边取对数后，可以得到我们定义的损失函数（添加L2正则）：\n",
    "$$\n",
    "J(\\alpha) = \\log{L(\\alpha)} + reg = \\sum_{i=1}^{m} y^{i} \\log {h_{\\alpha}(x^{i})} + (1-y^{i}) \\log {(1-h_{\\alpha}(x^{i}))} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_{j}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "&emsp;&emsp;接下来对损失函数 $J(\\alpha)$ 求梯度可以得到：\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{J(\\alpha)}}{\\partial{\\alpha_{j}}} &=&\n",
    "-\\frac{1}{m} \\sum_{i}^{m} \\big[y^i(1-h_{\\alpha}(x^i))*(-x_{j}^i)+(1-y^i)h_{\\alpha}(x^i)*(x_{j}^i) \\big] \\\\\n",
    "&=& -\\frac{1}{m} \\sum_{i}^{m} (y^i-h_{\\alpha}(x^{i})) * x_{j}^{i} \\\\\n",
    "&=& -\\frac{1}{m} \\sum_{i}^{m} \\big(h_{\\alpha}(x^{i}) - y^{i}) * x_{j}^{i} \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加正则项后，对于截距和非截距参数的梯度有：  \n",
    "截距参数：\n",
    "$$\n",
    "\\frac{\\partial{J(\\alpha)}}{\\partial{\\alpha_{0}}} = \n",
    "-\\frac{1}{m} \\sum_{i}^{m} \\big(h_{\\alpha}(x^{i}) - y^{i}) * x_{0}^{i}\n",
    "$$\n",
    "\n",
    "非截距参数：\n",
    "$$\n",
    "\\frac{\\partial{J(\\alpha)}}{\\partial{\\alpha_{j}}} = \n",
    "-\\frac{1}{m} \\sum_{i}^{m} \\big(h_{\\alpha}(x^{i}) - y^{i}) * x_{j}^{i} + \\frac{\\lambda}{m}\\alpha_{j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码示例\n",
    "&emsp;&emsp;OK，上面讲完了公式原理，我们现在依照上面的公式用python实现一下logistic regression。使用的数据集是8\\*8的手写数字集，仅选取了其中的数字0和数字1作为样本进行训练和验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随便看一个数字的样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9l+8Q5/pEyhkXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8912be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "target = digits.target\n",
    "\n",
    "plt.imshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理：\n",
    "- 选取出数字0和数字1样本；\n",
    "- 对数据进行归一化；\n",
    "- 随机划分训练和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_index = np.where(target == 0)[0]\n",
    "label_1_index = np.where(target == 1)[0]\n",
    "index = np.append(label_0_index, label_1_index)\n",
    "np.random.shuffle(index)\n",
    "data = data[index]\n",
    "target = target[index]\n",
    "scale = StandardScaler().fit(data)\n",
    "data = scale.transform(data)\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LR类，使用L2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LR(object):\n",
    "    def __init__(self, input_size, learning_rate=0.03, penalty=0.1, loop=10):\n",
    "        self.theta = np.random.randn(input_size + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.penalty = penalty\n",
    "        self.loop = loop\n",
    "\n",
    "    @staticmethod\n",
    "    def add_one(x):\n",
    "        return np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def hypothesis(self, x):\n",
    "        h = self.sigmoid(np.dot(x, self.theta))\n",
    "        return h\n",
    "\n",
    "    def cal_loss(self, h, x, y):\n",
    "        m = x.shape[0]\n",
    "        loss = 0\n",
    "        for i in range(m):\n",
    "            loss += - y[i] * np.log(h[i]) - (1 - y[i]) * np.log(1 - h[i])\n",
    "        loss = loss / m\n",
    "        reg = self.penalty * np.sum(pow(self.theta[1:], 2)) / (2 * m)\n",
    "        loss += reg\n",
    "        return loss\n",
    "\n",
    "    def cal_gradient(self, h, x, y):\n",
    "        m = x.shape[0]\n",
    "        gradient = (1.0 / m) * np.dot((h - y), x)\n",
    "        g0 = gradient[0]\n",
    "        reg = (self.penalty / m) * self.theta\n",
    "        gradient += reg\n",
    "        gradient[0] = g0\n",
    "        return gradient\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = self.add_one(x)\n",
    "        for i in range(self.loop):\n",
    "            h = self.hypothesis(x)\n",
    "            loss = self.cal_loss(h, x, y)\n",
    "            gradient = self.cal_gradient(h, x, y)\n",
    "            self.theta -= gradient * self.learning_rate\n",
    "            if i % 10 == 0:\n",
    "                print('step: {}, loss: {}'.format(i, loss))\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.add_one(x)\n",
    "        return np.around(self.hypothesis(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数：\n",
    "- 循环100次\n",
    "- 学习率0.03\n",
    "- 正则化lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 6.828746415814336\n",
      "step: 10, loss: 3.845734220130174\n",
      "step: 20, loss: 2.092951878744514\n",
      "step: 30, loss: 1.2679455302119425\n",
      "step: 40, loss: 0.8612109120034762\n",
      "step: 50, loss: 0.6392511184451278\n",
      "step: 60, loss: 0.5037327520378094\n",
      "step: 70, loss: 0.41308537094320574\n",
      "step: 80, loss: 0.34833322802432765\n",
      "step: 90, loss: 0.2998758322809995\n",
      "correct: 94, accuracy: 0.8703703703703703\n"
     ]
    }
   ],
   "source": [
    "lr = LR(train_x.shape[1], loop=100)\n",
    "lr.train(train_x, train_y)\n",
    "predict = lr.predict(test_x)\n",
    "correct = np.sum(predict == test_y)\n",
    "accuracy = correct / float(test_y.shape[0])\n",
    "print('correct: {}, accuracy: {}'.format(correct, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enjoy it !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}