{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- 这里我们学习一下pyTorch的Tensors基本数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.实现一个简单的神经网络反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 100\n",
    "# 输入维度\n",
    "input_size = 64\n",
    "# 隐层维度\n",
    "hidden_size = 1000\n",
    "# 输出维度\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正态分布随机定义训练数据\n",
    "# type可以将tensor转换成指定的数据格式\n",
    "x = torch.randn(batch_size, input_size).type(torch.FloatTensor)\n",
    "y = torch.randn(batch_size, output_size).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可训练参数定义\n",
    "w1 = torch.randn(input_size, hidden_size)\n",
    "w2 = torch.randn(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们看一下反向传播的计算步骤，图片截自课程[UFLDL](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n",
    "\n",
    "![反向传播计算步骤](https://github.com/nanyoullm/nanyoullm.github.io/blob/master/img/backpro.png?raw=true)\n",
    "<br\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 30453627.075980008\n",
      "step: 10, loss: 96118006.45569089\n",
      "step: 20, loss: 6532067.706399638\n",
      "step: 30, loss: 334688.0713296371\n",
      "step: 40, loss: 115934.43128277308\n",
      "step: 50, loss: 54652.07092030634\n",
      "step: 60, loss: 26871.274211959113\n",
      "step: 70, loss: 13525.872293335078\n",
      "step: 80, loss: 6936.796671634329\n",
      "step: 90, loss: 3612.269671562068\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # torch.mm（a, b）矩阵点乘操作\n",
    "    h = x.mm(w1)\n",
    "    # 实现relu，及取 x 和 0 之间的大值\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # 计算loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i % 10 == 0:\n",
    "        print('step: {}, loss: {}'.format(i, loss))\n",
    "    \n",
    "    # 计算梯度，激活层的函数为relu\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # 权重更新\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以上对pyTorch的Tensor操作有一个简单认识，如mm点乘操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyTorch有一个重要的对象，就是Variable，将Tensor转化为Variable后，pyTorch可以根据我们自定义的公式或者网络结构，实现自动梯度求导；\n",
    "\n",
    "<br\\>\n",
    "![variable](https://raw.githubusercontent.com/nanyoullm/nanyoullm.github.io/master/img/variable.png)\n",
    "<br\\>\n",
    "\n",
    "- 上图是Variable的重要属性。对于一个Variable对象，.data可以获得原始的tensor对象;当计算梯度后，该变量的梯度可以累计到，grad；\n",
    "- 我们先感受一下这个强大的功能，**很真实**；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们自定义一个$y$，$y=x+2$\n",
    "- 自定义一个$z$，$z=3*y^{2}$\n",
    "- $out=\\frac{1}{4}*z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "z = 3 * y * y\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们对out进行反向求导；\n",
    "- $out$对$x$的求导结果为：$\\frac{3}{2}*(x+2)$，$x$取矩阵中对应的值1，故为4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 请注意上面的结果，在out进行反向计算后，我们可以看到关系链最前段的x变量的梯度，但是我们无法看到y的梯度，返回了一个None，这是为什么呢？这里有一个合理的解释[pytorch_hook](https://www.zhihu.com/question/61044004)\n",
    "- pyTorch的开发者解释道：中间变量在完成了自身的反向传播使命后会被释放掉，因此我们想看中间变量的梯度，可以为其添加一个钩（hook)；直观理解，就是在这个中间变量完成反向传播计算的时候，再额外完成另一些任务，我们修改代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "[Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 用于记录y变量的梯度\n",
    "y_grads = []\n",
    "\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "y = x + 2\n",
    "z = 3 * y * y\n",
    "out = z.mean()\n",
    "# 在反向传播之前，为y注册一个hook，任务是记录梯度\n",
    "y.register_hook(lambda grad: y_grads.append(grad))\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 有公式我们知道，x和y的梯度应该是一样的；\n",
    "- 同时我们再思考一下，这个hook还有什么用呢？\n",
    "> 当你训练了一个网络，想提取中间层的参数或者CNN中的feature map时，hook就可以排上用场啦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 了解了Variable，现在我们使用Variable来重新实现上面的反向求导;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, input_size).type(torch.FloatTensor)\n",
    "x = Variable(x, requires_grad=False)\n",
    "y = torch.randn(batch_size, output_size).type(torch.FloatTensor)\n",
    "y = Variable(y, requires_grad=False)\n",
    "\n",
    "w1 = torch.randn(input_size, hidden_size).type(torch.FloatTensor)\n",
    "w1 = Variable(w1, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, output_size).type(torch.FloatTensor)\n",
    "w2 = Variable(w2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 30182246.0\n",
      "step: 10, loss: 110130184.0\n",
      "step: 20, loss: 6302686.0\n",
      "step: 30, loss: 294130.6875\n",
      "step: 40, loss: 114886.25\n",
      "step: 50, loss: 55154.7421875\n",
      "step: 60, loss: 27675.9296875\n",
      "step: 70, loss: 14308.21875\n",
      "step: 80, loss: 7567.3125\n",
      "step: 90, loss: 4075.057373046875\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i % 10 == 0:\n",
    "        print('step: {}, loss: {}'.format(i, loss.data[0]))\n",
    "    \n",
    "    loss.backward()\n",
    "    w1.data = w1.data - learning_rate * w1.grad.data\n",
    "    w2.data = w2.data - learning_rate * w2.grad.data\n",
    "    \n",
    "    # 注意！！！ 前面我们说到，Variable的grad是会累计的，所以每次计算之后需要清零\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前面我们说到，Variable的grad是会累计的，所以每次计算之后需要清零;\n",
    "- 另外Variable.data可以获取到tensor对象;\n",
    "- 通过自动计算梯度，我们可以免去自行推公式的繁琐，并且保证不会出错;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor VS Numpy\n",
    "pyTorch内设置了tensor和numpy的array的转换桥梁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# array to tensor\n",
    "a = torch.from_numpy(np.array([1, 2, 3]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# tensor to array\n",
    "b = torch.ones(2, 2).numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义autograd函数\n",
    "- pytorch里，用户可以自定义autograd函数，可以参考[autograd](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enjoy it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
