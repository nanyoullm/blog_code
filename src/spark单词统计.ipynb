{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用spark一个简单的单词统计，文本来源是spark预编译包里的README.md文档，虽然这是一个很小的文件，根本不需要使用spark。但是可以对rdd的操作有一个最基本的实践。  \n",
    "新建一个jupyter spark的kernel时，会自动新建一个变量sc，这是一个sparkContext对象，它是整个spark程序的入口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@1d533154"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先，我们读取文件，读取本地文件是可以使用sc.textFile()，将本地路径填写进去即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filePath = \"/home/lm/Softwares/spark-2.1.0-bin-hadoop2.6/README.md\"\n",
    "val data = sc.textFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们先看一下文本的前20行内容，可以发现，读取文件时一行数据在rdd里就是一条数据，共有104行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "MLlib for machine learning, GraphX for graph processing,\n",
      "and Spark Streaming for stream processing.\n",
      "\n",
      "<http://spark.apache.org/>\n",
      "\n",
      "\n",
      "## Online Documentation\n",
      "\n",
      "You can find the latest Spark documentation, including a programming\n",
      "guide, on the [project web page](http://spark.apache.org/documentation.html).\n",
      "This README file only contains basic setup instructions.\n",
      "\n",
      "## Building Spark\n",
      "\n",
      "-----------------------------\n",
      "数据总量： 104\n"
     ]
    }
   ],
   "source": [
    "data.take(20).foreach(println)\n",
    "println(\"-----------------------------\")\n",
    "println(\"数据总量： \" + data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们注意到，数据中有一些空行，对此我们可以过滤掉长度为0的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd1 = data.filter(_.length > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接着我们对每一行的句子进行split，符号为\" \"，这样可以取出每个独立的单词，那么数据就由 RDD[String] => RDD[List[String]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2 = rdd1.map(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接着我们使用flatMap操作将List[String]取出并且flat，这样数据又由 RDD[List[String]] => RDD[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd3 = rdd2.flatMap(x => x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们还发现，split后，每个单词里面有包含了标点\",\"和\".\"等等等，我们这里只暂时替换\",\"\".\"，对此我们进行一下处理，去掉这些符号;\n",
    "- 另外，单词中还包含纯标点符号，我们也要去掉这些脏数据；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val regex=\"^[A-z]*$\".r \n",
    "val rdd4 = rdd3.map(_.replace(\",\", \"\").replace(\".\", \"\")).filter(regex.findFirstMatchIn(_) != None).filter(_.length > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache\n",
      "Spark\n",
      "Spark\n",
      "is\n",
      "a\n",
      "fast\n",
      "and\n",
      "general\n",
      "cluster\n",
      "computing\n",
      "system\n",
      "for\n",
      "Big\n",
      "Data\n",
      "It\n",
      "provides\n",
      "APIs\n",
      "in\n",
      "Scala\n",
      "Java\n",
      "单词总数为: 436\n"
     ]
    }
   ],
   "source": [
    "rdd4.take(20).foreach(println)\n",
    "println(\"单词总数为: \" + rdd4.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接下来，我们把每个单词中的字母都统一为小写，然后把每个单词构造为key-value结构，key就是单词本身，value为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd5 = rdd4.map(x => (x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后进行reduceByKey，value值在reduceByKey时相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd6 = rdd5.reduceByKey{case(a, b) => a + b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到这里，我们已经对单词进行了统计，我们看一下结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(package,2)\n",
      "(this,1)\n",
      "(Because,1)\n",
      "(Python,4)\n",
      "(its,1)\n",
      "(guide,1)\n",
      "([run,1)\n",
      "(general,3)\n",
      "(have,1)\n",
      "(locally,3)\n",
      "出现的单词数量为： 224\n"
     ]
    }
   ],
   "source": [
    "rdd6.take(10).foreach(println)\n",
    "println(\"出现的单词数量为： \" + rdd6.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如package，这里统计出现了两次，我们看一下原始文件。\n",
    "<img src=\"https://raw.githubusercontent.com/nanyoullm/nanyoullm.github.io/master/img/spark-wordcount.png\">\n",
    "<br>\n",
    "- 原始文件出现了三次package，其中有一次出现的情况是\"package.)\"，我们replace了\".\"，而没有处理\")\"，故没有参与统计，可以做优化处理。\n",
    "- 如果我们想看下哪个单词出现频率最高，还需要对次数做一个排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd7 = rdd6.map(x => (x._2, x._1)).sortByKey(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,the)\n",
      "(17,Spark)\n",
      "(17,to)\n",
      "(12,for)\n",
      "(9,and)\n"
     ]
    }
   ],
   "source": [
    "rdd7.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们发现排名前五的单词如上所示。\n",
    "- 以上我们共创建了7个rdd，实际运用时可以使用链式法则将所有的transform操作直接串起来，这个就看个人喜好了。\n",
    "\n",
    "# enjoy it！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}